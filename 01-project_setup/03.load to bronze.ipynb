{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc65b8d-3dca-45ca-810f-e7e804c5e96a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"env\",defaultValue='',label='Enter the environment in lower case')\n",
    "env = dbutils.widgets.get(\"env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5421a8-d996-4364-b5a7-b20998469adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./commons\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817a5821-946a-41d4-bbe5-aba91eee97e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating a read_Traffic_Data() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34798a7b-bc85-498c-a2af-125d103ca98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_Traffic_Data():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "    print(\"Reading the Raw Traffic Data :  \", end='')\n",
    "    schema = StructType([\n",
    "        StructField(\"Record_ID\",IntegerType()),\n",
    "        StructField(\"Count_point_id\",IntegerType()),\n",
    "        StructField(\"Direction_of_travel\",StringType()),\n",
    "        StructField(\"Year\",IntegerType()),\n",
    "        StructField(\"Count_date\",StringType()),\n",
    "        StructField(\"hour\",IntegerType()),\n",
    "        StructField(\"Region_id\",IntegerType()),\n",
    "        StructField(\"Region_name\",StringType()),\n",
    "        StructField(\"Local_authority_name\",StringType()),\n",
    "        StructField(\"Road_name\",StringType()),\n",
    "        StructField(\"Road_Category_ID\",IntegerType()),\n",
    "        StructField(\"Start_junction_road_name\",StringType()),\n",
    "        StructField(\"End_junction_road_name\",StringType()),\n",
    "        StructField(\"Latitude\",DoubleType()),\n",
    "        StructField(\"Longitude\",DoubleType()),\n",
    "        StructField(\"Link_length_km\",DoubleType()),\n",
    "        StructField(\"Pedal_cycles\",IntegerType()),\n",
    "        StructField(\"Two_wheeled_motor_vehicles\",IntegerType()),\n",
    "        StructField(\"Cars_and_taxis\",IntegerType()),\n",
    "        StructField(\"Buses_and_coaches\",IntegerType()),\n",
    "        StructField(\"LGV_Type\",IntegerType()),\n",
    "        StructField(\"HGV_Type\",IntegerType()),\n",
    "        StructField(\"EV_Car\",IntegerType()),\n",
    "        StructField(\"EV_Bike\",IntegerType())\n",
    "        ])\n",
    "\n",
    "    rawTraffic_stream = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\",\"csv\")\n",
    "        .option('cloudFiles.schemaLocation',f'{checkpoint}/rawTrafficLoad/schemaInfer')\n",
    "        .option('header','true')\n",
    "        .schema(schema)\n",
    "        .load(landing+'/raw_traffic/')\n",
    "        .withColumn(\"Extract_Time\", current_timestamp())) # add timestamp to track the time\n",
    "    \n",
    "    print('Reading Succcess !!')\n",
    "    print('*******************')\n",
    "\n",
    "    return rawTraffic_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69cab728-6d5d-435c-abba-a85d83075965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating read_Road_Data() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6eb13d8-89b0-47f7-a52d-9a2941c3b92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_Road_Data():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "    print(\"Reading the Raw Roads Data :  \", end='')\n",
    "    schema = StructType([\n",
    "        StructField('Road_ID',IntegerType()),\n",
    "        StructField('Road_Category_Id',IntegerType()),\n",
    "        StructField('Road_Category',StringType()),\n",
    "        StructField('Region_ID',IntegerType()),\n",
    "        StructField('Region_Name',StringType()),\n",
    "        StructField('Total_Link_Length_Km',DoubleType()),\n",
    "        StructField('Total_Link_Length_Miles',DoubleType()),\n",
    "        StructField('All_Motor_Vehicles',DoubleType())\n",
    "        \n",
    "        ])\n",
    "\n",
    "    rawRoads_stream = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\",\"csv\")\n",
    "        .option('cloudFiles.schemaLocation',f'{checkpoint}/rawRoadsLoad/schemaInfer')\n",
    "        .option('header','true')\n",
    "        .schema(schema)\n",
    "        .load(landing+'/raw_roads/')\n",
    "        )\n",
    "    \n",
    "    print('Reading Succcess !!')\n",
    "    print('*******************')\n",
    "\n",
    "    return rawRoads_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c2e698-0731-4d94-b792-876fd14bc221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating write_Traffic_Data(StreamingDF,environment) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4514b0-b13e-498b-9643-f3792cc78942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Traffic_Data(StreamingDF,environment):\n",
    "    print(f'Writing data to {environment}_catalog raw_traffic table', end='' )\n",
    "    write_Stream = (StreamingDF.writeStream\n",
    "                    .format('delta')\n",
    "                    .option(\"checkpointLocation\",checkpoint + '/rawTrafficLoad/Checkpt')\n",
    "                    .outputMode('append')\n",
    "                    .queryName('rawTrafficWriteStream')\n",
    "                    .trigger(availableNow=True) # stops the streaming once data is written --> batch\n",
    "                    .toTable(f\"`{environment}_catalog`.`bronze`.`raw_traffic`\"))\n",
    "    \n",
    "    write_Stream.awaitTermination()\n",
    "    print('Write Success')\n",
    "    print(\"****************************\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1006d3-a837-48c3-8c20-5cec9b53d083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating write_Road_Data(StreamingDF,environment) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75f7da5-e36d-4d26-8606-1c5ab62414e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Road_Data(StreamingDF,environment):\n",
    "    print(f'Writing data to {environment}_catalog raw_roads table', end='' )\n",
    "    write_Data = (StreamingDF.writeStream\n",
    "                    .format('delta')\n",
    "                    .option(\"checkpointLocation\",checkpoint + '/rawRoadsLoad/Checkpt')\n",
    "                    .outputMode('append')\n",
    "                    .queryName('rawRoadsWriteStream')\n",
    "                    .trigger(availableNow=True) # stops the streaming once data is written --> batch\n",
    "                    .toTable(f\"`{environment}_catalog`.`bronze`.`raw_roads`\"))\n",
    "    \n",
    "    write_Data.awaitTermination()\n",
    "    print('Write Success')\n",
    "    print(\"****************************\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674384ee-e7cc-479f-a1cb-d71222d47ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Calling read and Write Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81dc1c09-5e79-46d0-a3cd-9f7bbde0a7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Reading the raw_traffic's data from landing to Bronze\n",
    "read_Df = read_Traffic_Data()\n",
    "\n",
    "## Reading the raw_roads's data from landing to Bronze\n",
    "read_roads = read_Road_Data()\n",
    "\n",
    "## Writing the raw_traffic's data from landing to Bronze\n",
    "write_Traffic_Data(read_Df,env)\n",
    "\n",
    "## Writing the raw_roads's data from landing to Bronze\n",
    "write_Road_Data(read_roads,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d176183b-1ce3-45cd-bd32-53daf3679e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT COUNT(*) FROM `{env}_catalog`.`bronze`.`raw_traffic` Limit 10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322c2aab-6b20-4ceb-8e8f-c8f1f46cd887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT COUNT(*) FROM `{env}_catalog`.`bronze`.`raw_roads` LIMIT 10\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03.load to bronze",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "21ca76e8-6300-42ea-bc17-13d5e1cc8897",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter the environment in lower case",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
